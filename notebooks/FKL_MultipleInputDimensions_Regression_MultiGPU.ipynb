{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import spectralgp\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5825bf2ab0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "seed = np.random.randint(42)\n",
    "print(seed)\n",
    "torch.random.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing training Data\n",
    "\n",
    "Here we setup a function to load training data as well as randomly split the data for train/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(dataset, only_scale = False, **kwargs):\n",
    "    D = loadmat(\"../exps_multi_input_dim/data/{}.mat\".format(dataset))\n",
    "    data = np.array(D['data'])\n",
    "    train_x, test_x, train_y, test_y = train_test_split(data[:, :-1], data[:, -1], test_size=0.10, random_state=np.random.randint(10000))\n",
    "    \n",
    "    test_x = (test_x - np.mean(train_x, axis=0))/(np.std(train_x, axis=0)+1e-17)\n",
    "    train_x = (train_x - np.mean(train_x, axis=0))/(np.std(train_x, axis=0)+1e-17)\n",
    "    \n",
    "    train_x = torch.tensor(train_x)\n",
    "    train_y = torch.tensor(train_y)\n",
    "    test_x = torch.tensor(test_x)\n",
    "    test_y = torch.tensor(test_y)\n",
    "\n",
    "    y_std = torch.std(torch.cat([train_y, test_y]))\n",
    "\n",
    "    return train_x, train_y, test_x, test_y, y_std, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: skillcraft2\n"
     ]
    }
   ],
   "source": [
    "dataset = 'skillcraft2'\n",
    "\n",
    "print(\"Dataset: {}\".format(dataset))\n",
    "train_x, train_y, test_x, test_y, y_std, gen_kern = read_data(dataset)\n",
    "in_dims = 1 if train_x.dim() == 1 else train_x.size(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU support\n",
    "\n",
    "If CUDA is available, we can simply just change the datatypes of the training/test tensor to the appropriate datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available True\n",
      "Input Dimensions 19\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print('Cuda is available', use_cuda)\n",
    "if use_cuda:\n",
    "    torch.set_default_tensor_type(torch.cuda.DoubleTensor)\n",
    "    train_x, train_y, test_x, test_y, y_std = train_x.cuda(), train_y.cuda(), test_x.cuda(), test_y.cuda(), y_std.cuda()\n",
    "    #if gen_kern is not None:\n",
    "    #    gen_kern = gen_kern.cuda()\n",
    "\n",
    "print(\"Input Dimensions {}\".format(in_dims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planning to run on 4 GPUs.\n"
     ]
    }
   ],
   "source": [
    "n_devices = torch.cuda.device_count()\n",
    "print('Planning to run on {} GPUs.'.format(n_devices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Set up the GPyTorch Model with Spectral Product GP kernel\n",
    "\n",
    "Using the same framework as standard GPyTorch models, we merely drop-in the product spectral GP kernel as the covar module.\n",
    "\n",
    "For specifics on the components of GPyTorch models we refer to the [GPyTorch Documentation](https://gpytorch.readthedocs.io/en/latest/index.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, n_devices, shared, **kwargs):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = spectralgp.kernels.ProductSpectralGPKernel(train_x, train_y, shared, **kwargs)\n",
    "        \n",
    "        self.gpu_module = gpytorch.kernels.MultiDeviceKernel(\n",
    "            self.covar_module,\n",
    "            device_ids=range(n_devices),\n",
    "            output_device=output_device\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.gpu_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1e-10, 8.0)\n",
      "(1e-10, 8.0)\n",
      "(1e-10, 8.0)\n",
      "(1e-10, 8.0)\n",
      "(1e-10, 8.0)\n",
      "(1e-10, 8.0)\n",
      "(1e-10, 8.0)\n",
      "(1e-10, 8.0)\n",
      "(1e-10, 8.0)\n",
      "(1e-10, 8.0)\n",
      "(1e-10, 8.0)\n",
      "(1e-10, 8.0)\n",
      "(1e-10, 8.0)\n",
      "(1e-10, 8.0)\n",
      "(1e-10, 8.0)\n",
      "(1e-10, 8.0)\n",
      "(1e-10, 8.0)\n",
      "(1e-10, 8.0)\n",
      "(1e-10, 8.0)\n"
     ]
    }
   ],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_prior=gpytorch.priors.SmoothedBoxPrior(1e-8, 1e-3))\n",
    "model = ExactGPModel(train_x, train_y, likelihood, shared=False, \n",
    "                     normalize = False, symmetrize = False, num_locs = 100,\n",
    "                     spacing='random', period_factor=36,\n",
    "                    n_devices = n_devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Sampling Factories and Alternating Sampler\n",
    "\n",
    "In the inference procedure we consider fixing the latent GP observation and doing gradient descent updates on the hyperparameters, then fixing the hyperparameters and using elliptical slice sampling to update the latent GP.\n",
    "\n",
    "The ss_factory generates a \"factory\" that fixes the latent GP and computes the loss function of the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_sampler = spectralgp.samplers.AlternatingSampler(\n",
    "    [model], [likelihood], \n",
    "    spectralgp.sampling_factories.ss_factory, [spectralgp.sampling_factories.ess_factory],\n",
    "    totalSamples=20, numInnerSamples=5, numOuterSamples=5,\n",
    "    num_dims=in_dims\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0 Dimension:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.9134264492090373 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-67.7858, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-67.0210, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-66.2741, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-65.5449, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-64.8334, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 0\n",
      "Step:  0 Dimension:  1\n",
      "Loss is:  tensor(-61.3658, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-60.6993, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-60.0482, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-59.4122, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-58.7913, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 0\n",
      "Step:  0 Dimension:  2\n",
      "Loss is:  tensor(-55.4079, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-54.8215, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-54.2483, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-53.6882, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-53.1411, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 0\n",
      "Step:  0 Dimension:  3\n",
      "Loss is:  tensor(-50.2424, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-49.7293, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-49.2275, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-48.7371, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-48.2578, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 0\n",
      "Step:  0 Dimension:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.9134264492090352 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-45.3966, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.913426449209033 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-44.9059, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.9134264492090189 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-44.4341, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.9134264492089974 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-44.0242, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.9134264492090238 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-43.6089, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 0\n",
      "Step:  0 Dimension:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.9134264492090187 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-41.0828, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.9134264492090296 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-40.6964, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.9134264492089996 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-40.3157, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.913426449209024 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-39.9508, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.9134264492090067 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-39.5878, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 0\n",
      "Step:  0 Dimension:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.9134264492088454 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-37.1045, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.9134264492088431 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-36.7666, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.9134264492089121 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-36.4356, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.9134264492088959 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-36.1127, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.9134264492087965 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-35.7963, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 0\n",
      "Step:  0 Dimension:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.9134264491968057 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-33.3771, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.9134264491947788 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-33.0815, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.9134264491949543 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-32.7926, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.9134264491943658 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-32.5096, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.9134264491921601 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-32.2326, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 0\n",
      "Step:  0 Dimension:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.9134264482647346 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-29.4818, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.9134264482309583 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-29.2240, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.9134264482281913 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-28.9732, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.9134264482954181 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-28.7266, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.9134264483041321 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-28.4858, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 0\n",
      "Step:  0 Dimension:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.9133995399188019 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-25.8977, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.9134231137649114 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-25.6652, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.9134164851628508 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-25.4505, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.9134153062995345 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-25.2294, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.9134247878271682 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-25.0094, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 0\n",
      "Step:  0 Dimension:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.004335438462835215 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-22.0352, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.004335438747373968 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-21.8264, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.00433542692832762 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-21.6446, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.00433544632951734 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-21.4441, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley_m/miniconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm 0.004335429793444908 which is larger than the tolerance of 0.001 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-21.2611, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 0\n",
      "Step:  0 Dimension:  11\n",
      "Loss is:  tensor(-18.9213, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-18.7598, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-18.5931, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-18.4265, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-18.2635, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 0\n",
      "Step:  0 Dimension:  12\n",
      "Loss is:  tensor(-16.1892, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-16.0375, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-15.8949, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-15.7493, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-15.6084, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 0\n",
      "Step:  0 Dimension:  13\n",
      "Loss is:  tensor(-13.2013, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-13.0713, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-12.9373, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-12.8161, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-12.6939, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 0\n",
      "Step:  0 Dimension:  14\n",
      "Loss is:  tensor(-10.9921, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-10.8788, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-10.7659, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-10.6514, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-10.5411, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 0\n",
      "Step:  0 Dimension:  15\n",
      "Loss is:  tensor(-8.6073, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-8.5035, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-8.4026, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-8.3048, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-8.2065, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 0\n",
      "Step:  0 Dimension:  16\n",
      "Loss is:  tensor(-6.7240, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-6.6318, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-6.5403, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-6.4509, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-6.3628, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 0\n",
      "Step:  0 Dimension:  17\n",
      "Loss is:  tensor(-6.0516, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-5.9672, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-5.8839, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-5.8020, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-5.7219, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 0\n",
      "Step:  0 Dimension:  18\n",
      "Loss is:  tensor(-5.5182, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-5.4433, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-5.3693, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-5.2967, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-5.2256, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 0\n",
      "Seconds for Iteration 0 : 817.8661813735962\n",
      "Step:  1 Dimension:  0\n",
      "Loss is:  tensor(-5.0520, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-4.9856, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-4.9199, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-4.8556, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-4.7927, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 1\n",
      "Step:  1 Dimension:  1\n",
      "Loss is:  tensor(-4.7221, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-4.6609, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-4.6007, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-4.5417, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-4.4839, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 1\n",
      "Step:  1 Dimension:  2\n",
      "Loss is:  tensor(-4.4245, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-4.3685, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-4.3133, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-4.2594, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-4.2066, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 1\n",
      "Step:  1 Dimension:  3\n",
      "Loss is:  tensor(-4.1415, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-4.0904, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-4.0399, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-3.9905, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-3.9422, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 1\n",
      "Step:  1 Dimension:  4\n",
      "Loss is:  tensor(-3.8825, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-3.8357, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-3.7891, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-3.7437, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-3.6994, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 1\n",
      "Step:  1 Dimension:  5\n",
      "Loss is:  tensor(-3.6452, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-3.6022, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-3.5595, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-3.5178, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-3.4771, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 1\n",
      "Step:  1 Dimension:  6\n",
      "Loss is:  tensor(-3.4203, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-3.3808, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-3.3415, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-3.3033, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-3.2661, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 1\n",
      "Step:  1 Dimension:  7\n",
      "Loss is:  tensor(-3.2114, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-3.1752, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-3.1393, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-3.1042, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-3.0699, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 1\n",
      "Step:  1 Dimension:  8\n",
      "Loss is:  tensor(-3.0280, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-2.9950, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-2.9617, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-2.9294, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-2.8982, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 1\n",
      "Step:  1 Dimension:  9\n",
      "Loss is:  tensor(-2.8603, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-2.8299, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-2.7996, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-2.7701, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-2.7415, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 1\n",
      "Step:  1 Dimension:  10\n",
      "Loss is:  tensor(-2.7073, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-2.6797, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-2.6519, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-2.6249, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-2.5989, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 1\n",
      "Step:  1 Dimension:  11\n",
      "Loss is:  tensor(-2.5606, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-2.5356, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-2.5101, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-2.4856, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-2.4619, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 1\n",
      "Step:  1 Dimension:  12\n",
      "Loss is:  tensor(-2.4287, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-2.4060, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-2.3829, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-2.3606, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-2.3393, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 1\n",
      "Step:  1 Dimension:  13\n",
      "Loss is:  tensor(-2.3045, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-2.2841, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-2.2631, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-2.2430, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-2.2237, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 1\n",
      "Step:  1 Dimension:  14\n",
      "Loss is:  tensor(-2.1961, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-2.1776, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-2.1585, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-2.1403, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-2.1228, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 1\n",
      "Step:  1 Dimension:  15\n",
      "Loss is:  tensor(-2.0847, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-2.0680, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-2.0506, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-2.0340, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-2.0182, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 1\n",
      "Step:  1 Dimension:  16\n",
      "Loss is:  tensor(-1.9927, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.9777, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.9619, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.9469, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.9327, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 1\n",
      "Step:  1 Dimension:  17\n",
      "Loss is:  tensor(-1.9139, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.9004, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.8862, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.8728, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.8601, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 1\n",
      "Step:  1 Dimension:  18\n",
      "Loss is:  tensor(-1.8362, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.8242, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.8113, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.7992, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.7879, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 1\n",
      "Seconds for Iteration 1 : 460.6863098144531\n",
      "Step:  2 Dimension:  0\n",
      "Loss is:  tensor(-1.7556, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-1.7449, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.7334, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.7226, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.7126, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 2\n",
      "Step:  2 Dimension:  1\n",
      "Loss is:  tensor(-1.6877, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.6781, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.6680, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.6584, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.6495, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 2\n",
      "Step:  2 Dimension:  2\n",
      "Loss is:  tensor(-1.6244, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.6159, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.6066, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.5978, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.5897, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 2\n",
      "Step:  2 Dimension:  3\n",
      "Loss is:  tensor(-1.5682, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.5607, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.5520, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.5440, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.5368, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 2\n",
      "Step:  2 Dimension:  4\n",
      "Loss is:  tensor(-1.5254, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.5188, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.5109, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.5037, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.4973, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 2\n",
      "Step:  2 Dimension:  5\n",
      "Loss is:  tensor(-1.4884, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.4821, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.4750, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.4686, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.4630, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 2\n",
      "Step:  2 Dimension:  6\n",
      "Loss is:  tensor(-1.4513, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.4448, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.4380, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.4321, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.4271, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 2\n",
      "Step:  2 Dimension:  7\n",
      "Loss is:  tensor(-1.4177, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.4126, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.4067, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.4018, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.3974, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 2\n",
      "Step:  2 Dimension:  8\n",
      "Loss is:  tensor(-1.3808, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.3764, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.3712, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.3665, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.3625, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 2\n",
      "Step:  2 Dimension:  9\n",
      "Loss is:  tensor(-1.3352, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.3314, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.3264, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.3220, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.3183, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 2\n",
      "Step:  2 Dimension:  10\n",
      "Loss is:  tensor(-1.3086, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.3053, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.3006, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.2967, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.2936, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 2\n",
      "Step:  2 Dimension:  11\n",
      "Loss is:  tensor(-1.2829, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.2799, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.2758, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.2723, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.2695, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 2\n",
      "Step:  2 Dimension:  12\n",
      "Loss is:  tensor(-1.2498, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.2473, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.2432, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.2400, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.2375, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 2\n",
      "Step:  2 Dimension:  13\n",
      "Loss is:  tensor(-1.2171, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.2148, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.2110, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.2080, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.2058, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 2\n",
      "Step:  2 Dimension:  14\n",
      "Loss is:  tensor(-1.1905, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.1885, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.1849, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.1821, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.1800, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 2\n",
      "Step:  2 Dimension:  15\n",
      "Loss is:  tensor(-1.1654, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.1637, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.1602, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.1576, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.1559, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 2\n",
      "Step:  2 Dimension:  16\n",
      "Loss is:  tensor(-1.1379, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.1362, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.1327, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.1303, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.1288, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 2\n",
      "Step:  2 Dimension:  17\n",
      "Loss is:  tensor(-1.1149, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.1133, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.1096, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.1070, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.1052, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 2\n",
      "Step:  2 Dimension:  18\n",
      "Loss is:  tensor(-1.0962, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0949, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0915, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0892, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0879, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 2\n",
      "Seconds for Iteration 2 : 456.1971983909607\n",
      "Step:  3 Dimension:  0\n",
      "Loss is:  tensor(-1.0808, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0798, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0771, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0752, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0741, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 3\n",
      "Step:  3 Dimension:  1\n",
      "Loss is:  tensor(-1.0728, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0718, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0688, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0667, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0657, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 3\n",
      "Step:  3 Dimension:  2\n",
      "Loss is:  tensor(-1.0554, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0549, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0522, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0501, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0496, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 3\n",
      "Step:  3 Dimension:  3\n",
      "Loss is:  tensor(-1.0433, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0428, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0399, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0382, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0375, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 3\n",
      "Step:  3 Dimension:  4\n",
      "Loss is:  tensor(-1.0316, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0315, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0291, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0276, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0270, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 3\n",
      "Step:  3 Dimension:  5\n",
      "Loss is:  tensor(-1.0282, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0281, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0255, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0241, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0238, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 3\n",
      "Step:  3 Dimension:  6\n",
      "Loss is:  tensor(-1.0206, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0198, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0181, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0167, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0162, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 3\n",
      "Step:  3 Dimension:  7\n",
      "Loss is:  tensor(-1.0162, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0168, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0143, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0135, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0130, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 3\n",
      "Step:  3 Dimension:  8\n",
      "Loss is:  tensor(-1.0131, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0126, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0108, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0089, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0088, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  3 Dimension:  9\n",
      "Loss is:  tensor(-1.0013, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-1.0017, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9999, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9982, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9981, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 3\n",
      "Step:  3 Dimension:  10\n",
      "Loss is:  tensor(-0.9943, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9944, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9920, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9910, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9905, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 3\n",
      "Step:  3 Dimension:  11\n",
      "Loss is:  tensor(-0.9915, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9922, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9901, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9888, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9889, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 3\n",
      "Step:  3 Dimension:  12\n",
      "Loss is:  tensor(-0.9899, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9904, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9883, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9873, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9877, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 3\n",
      "Step:  3 Dimension:  13\n",
      "Loss is:  tensor(-0.9824, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9836, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9809, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9795, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9803, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 3\n",
      "Step:  3 Dimension:  14\n",
      "Loss is:  tensor(-0.9773, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9783, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9764, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9749, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9758, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 3\n",
      "Step:  3 Dimension:  15\n",
      "Loss is:  tensor(-0.9732, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9747, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9719, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9708, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9712, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 3\n",
      "Step:  3 Dimension:  16\n",
      "Loss is:  tensor(-0.9721, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9728, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9711, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9693, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9699, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 3\n",
      "Step:  3 Dimension:  17\n",
      "Loss is:  tensor(-0.9627, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9638, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9615, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9605, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9615, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 3\n",
      "Step:  3 Dimension:  18\n",
      "Loss is:  tensor(-0.9573, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9589, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9567, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9559, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9563, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 3\n",
      "Seconds for Iteration 3 : 706.666068315506\n",
      "Step:  4 Dimension:  0\n",
      "Loss is:  tensor(-0.9424, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9439, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9412, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9405, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9408, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 4\n",
      "Step:  4 Dimension:  1\n",
      "Loss is:  tensor(-0.9422, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9438, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9410, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9400, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9407, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 4\n",
      "Step:  4 Dimension:  2\n",
      "Loss is:  tensor(-0.9375, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9386, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9368, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9351, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9364, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 4\n",
      "Step:  4 Dimension:  3\n",
      "Loss is:  tensor(-0.9329, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9344, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9323, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9312, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9313, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 4\n",
      "Step:  4 Dimension:  4\n",
      "Loss is:  tensor(-0.9302, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9308, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9289, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9281, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9282, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 4\n",
      "Step:  4 Dimension:  5\n",
      "Loss is:  tensor(-0.9264, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9275, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9258, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9254, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9254, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 4\n",
      "Step:  4 Dimension:  6\n",
      "Loss is:  tensor(-0.9239, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9254, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9236, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9222, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9227, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 4\n",
      "Step:  4 Dimension:  7\n",
      "Loss is:  tensor(-0.9214, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9231, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9207, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9198, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9214, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 4\n",
      "Step:  4 Dimension:  8\n",
      "Loss is:  tensor(-0.9203, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9219, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9193, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9190, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9194, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 4\n",
      "Step:  4 Dimension:  9\n",
      "Loss is:  tensor(-0.9168, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9185, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9160, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9145, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9168, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 4\n",
      "Step:  4 Dimension:  10\n",
      "Loss is:  tensor(-0.9121, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9144, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9110, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9099, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9110, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 4\n",
      "Step:  4 Dimension:  11\n",
      "Loss is:  tensor(-0.9088, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9111, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9073, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9073, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9072, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 4\n",
      "Step:  4 Dimension:  12\n",
      "Loss is:  tensor(-0.9064, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9076, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9057, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9049, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9056, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 4\n",
      "Step:  4 Dimension:  13\n",
      "Loss is:  tensor(-0.9034, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9070, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9042, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9024, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9040, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 4\n",
      "Step:  4 Dimension:  14\n",
      "Loss is:  tensor(-0.9041, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9064, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9037, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9028, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9036, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 4\n",
      "Step:  4 Dimension:  15\n",
      "Loss is:  tensor(-0.9000, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9021, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9000, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8979, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8993, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 4\n",
      "Step:  4 Dimension:  16\n",
      "Loss is:  tensor(-0.8992, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9011, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8987, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8984, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8987, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 4\n",
      "Step:  4 Dimension:  17\n",
      "Loss is:  tensor(-0.8971, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8999, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8976, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-0.8961, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8976, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 4\n",
      "Step:  4 Dimension:  18\n",
      "Loss is:  tensor(-0.8983, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9007, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8980, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8975, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8984, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 4\n",
      "Seconds for Iteration 4 : 976.127311706543\n",
      "Step:  5 Dimension:  0\n",
      "Loss is:  tensor(-0.8941, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8980, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8934, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8927, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8938, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 5\n",
      "Step:  5 Dimension:  1\n",
      "Loss is:  tensor(-0.8979, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.9001, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8969, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8965, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8982, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 5\n",
      "Step:  5 Dimension:  2\n",
      "Loss is:  tensor(-0.8975, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8982, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8962, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8957, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8970, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 5\n",
      "Step:  5 Dimension:  3\n",
      "Loss is:  tensor(-0.8958, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8968, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8950, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8918, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8944, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 5\n",
      "Step:  5 Dimension:  4\n",
      "Loss is:  tensor(-0.8905, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8924, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8905, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8891, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8921, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 5\n",
      "Step:  5 Dimension:  5\n",
      "Loss is:  tensor(-0.8875, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8896, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8859, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8850, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8858, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 5\n",
      "Step:  5 Dimension:  6\n",
      "Loss is:  tensor(-0.8858, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8892, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8847, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8834, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8857, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 5\n",
      "Step:  5 Dimension:  7\n",
      "Loss is:  tensor(-0.8827, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8847, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8810, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8795, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8820, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 5\n",
      "Step:  5 Dimension:  8\n",
      "Loss is:  tensor(-0.8794, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8823, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8781, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8778, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8791, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 5\n",
      "Step:  5 Dimension:  9\n",
      "Loss is:  tensor(-0.8777, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8820, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8776, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8772, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8787, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 5\n",
      "Step:  5 Dimension:  10\n",
      "Loss is:  tensor(-0.8783, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8791, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8775, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8760, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8762, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 5\n",
      "Step:  5 Dimension:  11\n",
      "Loss is:  tensor(-0.8754, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8780, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8753, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8743, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8750, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 5\n",
      "Step:  5 Dimension:  12\n",
      "Loss is:  tensor(-0.8753, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8781, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8746, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8734, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8744, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 5\n",
      "Step:  5 Dimension:  13\n",
      "Loss is:  tensor(-0.8785, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8795, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8773, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8755, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8773, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 5\n",
      "Step:  5 Dimension:  14\n",
      "Loss is:  tensor(-0.8776, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8805, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8759, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8760, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8778, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 5\n",
      "Step:  5 Dimension:  15\n",
      "Loss is:  tensor(-0.8803, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8812, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8773, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8777, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8781, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 5\n",
      "Step:  5 Dimension:  16\n",
      "Loss is:  tensor(-0.8786, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8822, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8783, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8777, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8783, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 5\n",
      "Step:  5 Dimension:  17\n",
      "Loss is:  tensor(-0.8770, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8795, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8768, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8756, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8768, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 5\n",
      "Step:  5 Dimension:  18\n",
      "Loss is:  tensor(-0.8766, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8808, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8766, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8754, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8771, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 5\n",
      "Seconds for Iteration 5 : 882.280012845993\n",
      "Step:  6 Dimension:  0\n",
      "Loss is:  tensor(-0.8739, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8763, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8744, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8720, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8744, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 6\n",
      "Step:  6 Dimension:  1\n",
      "Loss is:  tensor(-0.8729, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8760, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8719, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8713, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8722, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 6\n",
      "Step:  6 Dimension:  2\n",
      "Loss is:  tensor(-0.8724, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8749, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8705, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8713, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8734, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 6\n",
      "Step:  6 Dimension:  3\n",
      "Loss is:  tensor(-0.8728, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8760, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8716, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8711, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8729, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 6\n",
      "Step:  6 Dimension:  4\n",
      "Loss is:  tensor(-0.8738, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8761, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8731, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8726, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8742, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 6\n",
      "Step:  6 Dimension:  5\n",
      "Loss is:  tensor(-0.8755, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8786, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8743, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8747, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8757, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 6\n",
      "Step:  6 Dimension:  6\n",
      "Loss is:  tensor(-0.8727, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8754, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8725, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8709, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8734, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 6\n",
      "Step:  6 Dimension:  7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-0.8734, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8764, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8714, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8718, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8749, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 6\n",
      "Step:  6 Dimension:  8\n",
      "Loss is:  tensor(-0.8706, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8725, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8689, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8689, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8704, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 6\n",
      "Step:  6 Dimension:  9\n",
      "Loss is:  tensor(-0.8695, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8728, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8705, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8682, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8692, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 6\n",
      "Step:  6 Dimension:  10\n",
      "Loss is:  tensor(-0.8661, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8684, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8659, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8651, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8666, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 6\n",
      "Step:  6 Dimension:  11\n",
      "Loss is:  tensor(-0.8654, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8674, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8651, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8618, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8651, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 6\n",
      "Step:  6 Dimension:  12\n",
      "Loss is:  tensor(-0.8676, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8692, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8663, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8647, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8669, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 6\n",
      "Step:  6 Dimension:  13\n",
      "Loss is:  tensor(-0.8642, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8678, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8652, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8637, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8645, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 6\n",
      "Step:  6 Dimension:  14\n",
      "Loss is:  tensor(-0.8644, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8680, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8626, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8623, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8632, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 6\n",
      "Step:  6 Dimension:  15\n",
      "Loss is:  tensor(-0.8646, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8680, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8652, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8638, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8645, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 6\n",
      "Step:  6 Dimension:  16\n",
      "Loss is:  tensor(-0.8642, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8683, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8639, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8647, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8653, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 6\n",
      "Step:  6 Dimension:  17\n",
      "Loss is:  tensor(-0.8678, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8707, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8664, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8650, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8691, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 6\n",
      "Step:  6 Dimension:  18\n",
      "Loss is:  tensor(-0.8663, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8678, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8661, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8651, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8645, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 6\n",
      "Seconds for Iteration 6 : 865.4255840778351\n",
      "Step:  7 Dimension:  0\n",
      "Loss is:  tensor(-0.8705, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8697, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8680, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8669, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8675, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 7\n",
      "Step:  7 Dimension:  1\n",
      "Loss is:  tensor(-0.8701, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8735, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8696, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8688, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8708, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 7\n",
      "Step:  7 Dimension:  2\n",
      "Loss is:  tensor(-0.8706, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8750, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8687, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8690, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8713, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 7\n",
      "Step:  7 Dimension:  3\n",
      "Loss is:  tensor(-0.8673, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8709, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8669, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8658, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8678, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 7\n",
      "Step:  7 Dimension:  4\n",
      "Loss is:  tensor(-0.8670, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8674, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8647, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8639, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8650, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 7\n",
      "Step:  7 Dimension:  5\n",
      "Loss is:  tensor(-0.8642, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8648, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8638, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8630, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8644, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 7\n",
      "Step:  7 Dimension:  6\n",
      "Loss is:  tensor(-0.8666, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8705, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8665, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8653, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8673, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 7\n",
      "Step:  7 Dimension:  7\n",
      "Loss is:  tensor(-0.8633, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8670, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8636, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8620, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8645, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 7\n",
      "Step:  7 Dimension:  8\n",
      "Loss is:  tensor(-0.8609, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8613, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8593, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8573, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8586, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 7\n",
      "Step:  7 Dimension:  9\n",
      "Loss is:  tensor(-0.8597, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8651, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8616, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8594, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8620, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 7\n",
      "Step:  7 Dimension:  10\n",
      "Loss is:  tensor(-0.8585, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8614, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8583, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8562, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8577, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 7\n",
      "Step:  7 Dimension:  11\n",
      "Loss is:  tensor(-0.8557, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8591, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8562, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8538, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8553, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 7\n",
      "Step:  7 Dimension:  12\n",
      "Loss is:  tensor(-0.8550, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8593, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8549, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8548, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8568, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 7\n",
      "Step:  7 Dimension:  13\n",
      "Loss is:  tensor(-0.8542, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8586, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8548, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8540, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8539, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 7\n",
      "Step:  7 Dimension:  14\n",
      "Loss is:  tensor(-0.8504, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8546, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8510, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8499, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8513, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 7\n",
      "Step:  7 Dimension:  15\n",
      "Loss is:  tensor(-0.8483, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8523, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8464, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8466, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-0.8476, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 7\n",
      "Step:  7 Dimension:  16\n",
      "Loss is:  tensor(-0.8490, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8540, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8492, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8482, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8497, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 7\n",
      "Step:  7 Dimension:  17\n",
      "Loss is:  tensor(-0.8542, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8578, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8541, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8515, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8525, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 7\n",
      "Step:  7 Dimension:  18\n",
      "Loss is:  tensor(-0.8503, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8514, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8500, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8475, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8498, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 7\n",
      "Seconds for Iteration 7 : 865.655431509018\n",
      "Step:  8 Dimension:  0\n",
      "Loss is:  tensor(-0.8498, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8533, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8488, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8474, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8497, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 8\n",
      "Step:  8 Dimension:  1\n",
      "Loss is:  tensor(-0.8477, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8508, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8464, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8446, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8487, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 8\n",
      "Step:  8 Dimension:  2\n",
      "Loss is:  tensor(-0.8463, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8495, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8457, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8440, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8466, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 8\n",
      "Step:  8 Dimension:  3\n",
      "Loss is:  tensor(-0.8471, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8526, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8484, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8487, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8499, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 8\n",
      "Step:  8 Dimension:  4\n",
      "Loss is:  tensor(-0.8477, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8524, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8471, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8476, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8496, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 8\n",
      "Step:  8 Dimension:  5\n",
      "Loss is:  tensor(-0.8468, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8497, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8463, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8448, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8475, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 8\n",
      "Step:  8 Dimension:  6\n",
      "Loss is:  tensor(-0.8439, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8487, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8451, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8429, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8450, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 8\n",
      "Step:  8 Dimension:  7\n",
      "Loss is:  tensor(-0.8421, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8457, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8425, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8409, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8441, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 8\n",
      "Step:  8 Dimension:  8\n",
      "Loss is:  tensor(-0.8427, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8466, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8439, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8410, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8440, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 8\n",
      "Step:  8 Dimension:  9\n",
      "Loss is:  tensor(-0.8413, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8452, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8419, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8402, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8426, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 8\n",
      "Step:  8 Dimension:  10\n",
      "Loss is:  tensor(-0.8429, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8470, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8435, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8407, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8422, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 8\n",
      "Step:  8 Dimension:  11\n",
      "Loss is:  tensor(-0.8427, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8469, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8425, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8411, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8428, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 8\n",
      "Step:  8 Dimension:  12\n",
      "Loss is:  tensor(-0.8414, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8461, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8411, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8402, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8422, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 8\n",
      "Step:  8 Dimension:  13\n",
      "Loss is:  tensor(-0.8404, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8429, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8395, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8367, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8402, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 8\n",
      "Step:  8 Dimension:  14\n",
      "Loss is:  tensor(-0.8433, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8464, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8419, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8410, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8432, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 8\n",
      "Step:  8 Dimension:  15\n",
      "Loss is:  tensor(-0.8435, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8475, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8408, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8417, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8431, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 8\n",
      "Step:  8 Dimension:  16\n",
      "Loss is:  tensor(-0.8450, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8484, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8437, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8430, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8442, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 8\n",
      "Step:  8 Dimension:  17\n",
      "Loss is:  tensor(-0.8408, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8443, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8404, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8397, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8394, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 8\n",
      "Step:  8 Dimension:  18\n",
      "Loss is:  tensor(-0.8368, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8438, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8370, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8384, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8397, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 8\n",
      "Seconds for Iteration 8 : 836.219390630722\n",
      "Step:  9 Dimension:  0\n",
      "Loss is:  tensor(-0.8406, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8411, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8377, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8369, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8404, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 9\n",
      "Step:  9 Dimension:  1\n",
      "Loss is:  tensor(-0.8395, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8421, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8372, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8367, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8397, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 9\n",
      "Step:  9 Dimension:  2\n",
      "Loss is:  tensor(-0.8361, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8411, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8346, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8345, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8368, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 9\n",
      "Step:  9 Dimension:  3\n",
      "Loss is:  tensor(-0.8338, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8361, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8320, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8320, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8348, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 9\n",
      "Step:  9 Dimension:  4\n",
      "Loss is:  tensor(-0.8403, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8452, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8397, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8389, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8401, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 9\n",
      "Step:  9 Dimension:  5\n",
      "Loss is:  tensor(-0.8414, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-0.8432, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8407, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8392, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8404, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 9\n",
      "Step:  9 Dimension:  6\n",
      "Loss is:  tensor(-0.8419, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8442, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8407, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8397, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8415, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 9\n",
      "Step:  9 Dimension:  7\n",
      "Loss is:  tensor(-0.8448, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8467, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8431, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8417, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8437, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 9\n",
      "Step:  9 Dimension:  8\n",
      "Loss is:  tensor(-0.8431, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8470, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8430, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8398, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8424, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 9\n",
      "Step:  9 Dimension:  9\n",
      "Loss is:  tensor(-0.8424, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8450, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8431, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8409, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8422, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 9\n",
      "Step:  9 Dimension:  10\n",
      "Loss is:  tensor(-0.8446, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8477, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8433, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8429, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8440, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 9\n",
      "Step:  9 Dimension:  11\n",
      "Loss is:  tensor(-0.8484, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8527, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8474, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8476, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8490, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 9\n",
      "Step:  9 Dimension:  12\n",
      "Loss is:  tensor(-0.8499, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8514, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8483, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8465, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8480, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 9\n",
      "Step:  9 Dimension:  13\n",
      "Loss is:  tensor(-0.8537, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8563, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8504, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8488, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8516, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 9\n",
      "Step:  9 Dimension:  14\n",
      "Loss is:  tensor(-0.8494, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8517, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8481, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8467, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8487, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 9\n",
      "Step:  9 Dimension:  15\n",
      "Loss is:  tensor(-0.8498, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8549, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8493, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8481, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8494, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 9\n",
      "Step:  9 Dimension:  16\n",
      "Loss is:  tensor(-0.8491, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8535, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8484, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8474, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8506, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 9\n",
      "Step:  9 Dimension:  17\n",
      "Loss is:  tensor(-0.8510, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8556, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8513, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8487, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8521, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 9\n",
      "Step:  9 Dimension:  18\n",
      "Loss is:  tensor(-0.8491, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8489, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8465, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8439, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8473, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 9\n",
      "Seconds for Iteration 9 : 789.0026659965515\n",
      "Step:  10 Dimension:  0\n",
      "Loss is:  tensor(-0.8466, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8492, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8436, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8430, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8448, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 10\n",
      "Step:  10 Dimension:  1\n",
      "Loss is:  tensor(-0.8436, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8479, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8431, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8430, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8452, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 10\n",
      "Step:  10 Dimension:  2\n",
      "Loss is:  tensor(-0.8445, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8473, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8443, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8417, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8431, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 10\n",
      "Step:  10 Dimension:  3\n",
      "Loss is:  tensor(-0.8423, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8480, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8426, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8422, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8441, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 10\n",
      "Step:  10 Dimension:  4\n",
      "Loss is:  tensor(-0.8427, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8469, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8400, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8409, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8428, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 10\n",
      "Step:  10 Dimension:  5\n",
      "Loss is:  tensor(-0.8394, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8415, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8379, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8360, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8391, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 10\n",
      "Step:  10 Dimension:  6\n",
      "Loss is:  tensor(-0.8393, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8442, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8405, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8377, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8397, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 10\n",
      "Step:  10 Dimension:  7\n",
      "Loss is:  tensor(-0.8390, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8422, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8373, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8396, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8406, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 10\n",
      "Step:  10 Dimension:  8\n",
      "Loss is:  tensor(-0.8359, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8383, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8350, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8326, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8347, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 10\n",
      "Step:  10 Dimension:  9\n",
      "Loss is:  tensor(-0.8377, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8425, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8359, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8344, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8377, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 10\n",
      "Step:  10 Dimension:  10\n",
      "Loss is:  tensor(-0.8350, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8401, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8346, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8322, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8364, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 10\n",
      "Step:  10 Dimension:  11\n",
      "Loss is:  tensor(-0.8354, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8373, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8336, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8306, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8325, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 10\n",
      "Step:  10 Dimension:  12\n",
      "Loss is:  tensor(-0.8316, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8355, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8316, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8308, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8345, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 10\n",
      "Step:  10 Dimension:  13\n",
      "Loss is:  tensor(-0.8338, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8369, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8327, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8309, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-0.8334, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 10\n",
      "Step:  10 Dimension:  14\n",
      "Loss is:  tensor(-0.8332, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8354, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8319, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8304, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8324, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 10\n",
      "Step:  10 Dimension:  15\n",
      "Loss is:  tensor(-0.8301, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8337, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8292, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8282, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8285, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 10\n",
      "Step:  10 Dimension:  16\n",
      "Loss is:  tensor(-0.8306, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8331, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8293, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8287, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8303, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 10\n",
      "Step:  10 Dimension:  17\n",
      "Loss is:  tensor(-0.8298, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8324, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8282, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8248, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8295, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 10\n",
      "Step:  10 Dimension:  18\n",
      "Loss is:  tensor(-0.8276, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8338, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8268, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8265, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8279, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 10\n",
      "Seconds for Iteration 10 : 851.2605438232422\n",
      "Step:  11 Dimension:  0\n",
      "Loss is:  tensor(-0.8295, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8327, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8288, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8276, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8304, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 11\n",
      "Step:  11 Dimension:  1\n",
      "Loss is:  tensor(-0.8273, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8303, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8270, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8254, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8264, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 11\n",
      "Step:  11 Dimension:  2\n",
      "Loss is:  tensor(-0.8312, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8372, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8280, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8284, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8325, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 11\n",
      "Step:  11 Dimension:  3\n",
      "Loss is:  tensor(-0.8309, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8346, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8291, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8284, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8300, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 11\n",
      "Step:  11 Dimension:  4\n",
      "Loss is:  tensor(-0.8315, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8360, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8293, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8290, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8317, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 11\n",
      "Step:  11 Dimension:  5\n",
      "Loss is:  tensor(-0.8284, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8334, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8295, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8272, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8313, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 11\n",
      "Step:  11 Dimension:  6\n",
      "Loss is:  tensor(-0.8288, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8322, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8268, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8267, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8271, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 11\n",
      "Step:  11 Dimension:  7\n",
      "Loss is:  tensor(-0.8244, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8269, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8234, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8217, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8240, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 11\n",
      "Step:  11 Dimension:  8\n",
      "Loss is:  tensor(-0.8241, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8260, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8225, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8225, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8233, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 11\n",
      "Step:  11 Dimension:  9\n",
      "Loss is:  tensor(-0.8208, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8276, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8192, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8204, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8246, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 11\n",
      "Step:  11 Dimension:  10\n",
      "Loss is:  tensor(-0.8202, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8297, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8234, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8220, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8224, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 11\n",
      "Step:  11 Dimension:  11\n",
      "Loss is:  tensor(-0.8261, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8294, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8260, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8249, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8262, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 11\n",
      "Step:  11 Dimension:  12\n",
      "Loss is:  tensor(-0.8233, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8271, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8220, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8198, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8223, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 11\n",
      "Step:  11 Dimension:  13\n",
      "Loss is:  tensor(-0.8234, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8280, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8222, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8223, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8232, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 11\n",
      "Step:  11 Dimension:  14\n",
      "Loss is:  tensor(-0.8209, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8257, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8207, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8192, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8205, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 11\n",
      "Step:  11 Dimension:  15\n",
      "Loss is:  tensor(-0.8188, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8250, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8201, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8186, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8216, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 11\n",
      "Step:  11 Dimension:  16\n",
      "Loss is:  tensor(-0.8199, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8234, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8191, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8181, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8198, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 11\n",
      "Step:  11 Dimension:  17\n",
      "Loss is:  tensor(-0.8187, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8228, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8195, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8149, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8196, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 11\n",
      "Step:  11 Dimension:  18\n",
      "Loss is:  tensor(-0.8191, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8242, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8185, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8156, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8185, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 11\n",
      "Seconds for Iteration 11 : 840.9638452529907\n",
      "Step:  12 Dimension:  0\n",
      "Loss is:  tensor(-0.8189, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8224, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8161, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8154, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8191, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 12\n",
      "Step:  12 Dimension:  1\n",
      "Loss is:  tensor(-0.8132, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8188, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8127, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8121, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8147, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 12\n",
      "Step:  12 Dimension:  2\n",
      "Loss is:  tensor(-0.8144, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8182, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8120, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8097, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8117, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  12 Dimension:  3\n",
      "Loss is:  tensor(-0.8106, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8135, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8075, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8048, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8083, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 12\n",
      "Step:  12 Dimension:  4\n",
      "Loss is:  tensor(-0.8075, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8118, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8076, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8068, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8086, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 12\n",
      "Step:  12 Dimension:  5\n",
      "Loss is:  tensor(-0.8118, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8153, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8113, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8089, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8111, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 12\n",
      "Step:  12 Dimension:  6\n",
      "Loss is:  tensor(-0.8106, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8142, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8106, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8070, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8105, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 12\n",
      "Step:  12 Dimension:  7\n",
      "Loss is:  tensor(-0.8109, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8133, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8087, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8062, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8079, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 12\n",
      "Step:  12 Dimension:  8\n",
      "Loss is:  tensor(-0.8072, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8129, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8077, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8073, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8094, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 12\n",
      "Step:  12 Dimension:  9\n",
      "Loss is:  tensor(-0.8060, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8074, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8026, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8006, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8040, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 12\n",
      "Step:  12 Dimension:  10\n",
      "Loss is:  tensor(-0.8070, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8099, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8051, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8030, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8070, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 12\n",
      "Step:  12 Dimension:  11\n",
      "Loss is:  tensor(-0.8076, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8123, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8060, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8052, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8070, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 12\n",
      "Step:  12 Dimension:  12\n",
      "Loss is:  tensor(-0.8090, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8131, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8063, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8050, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8105, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 12\n",
      "Step:  12 Dimension:  13\n",
      "Loss is:  tensor(-0.8064, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8107, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8028, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8021, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8051, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 12\n",
      "Step:  12 Dimension:  14\n",
      "Loss is:  tensor(-0.8057, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8089, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8045, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8029, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8062, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 12\n",
      "Step:  12 Dimension:  15\n",
      "Loss is:  tensor(-0.8113, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8146, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8100, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8082, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8114, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 12\n",
      "Step:  12 Dimension:  16\n",
      "Loss is:  tensor(-0.8047, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8084, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8044, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8028, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8045, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 12\n",
      "Step:  12 Dimension:  17\n",
      "Loss is:  tensor(-0.8045, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8092, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8025, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8028, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8036, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 12\n",
      "Step:  12 Dimension:  18\n",
      "Loss is:  tensor(-0.8008, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8057, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8003, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7979, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8018, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 12\n",
      "Seconds for Iteration 12 : 742.3255753517151\n",
      "Step:  13 Dimension:  0\n",
      "Loss is:  tensor(-0.7974, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8007, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7979, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7960, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7983, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 13\n",
      "Step:  13 Dimension:  1\n",
      "Loss is:  tensor(-0.7973, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8017, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7968, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7956, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7987, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 13\n",
      "Step:  13 Dimension:  2\n",
      "Loss is:  tensor(-0.7996, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8034, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7990, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7957, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7994, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 13\n",
      "Step:  13 Dimension:  3\n",
      "Loss is:  tensor(-0.7973, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8043, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7971, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7948, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7979, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 13\n",
      "Step:  13 Dimension:  4\n",
      "Loss is:  tensor(-0.7987, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8031, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7995, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7960, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7983, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 13\n",
      "Step:  13 Dimension:  5\n",
      "Loss is:  tensor(-0.7995, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8034, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7980, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7980, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8001, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 13\n",
      "Step:  13 Dimension:  6\n",
      "Loss is:  tensor(-0.7998, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8029, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7980, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7983, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7991, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 13\n",
      "Step:  13 Dimension:  7\n",
      "Loss is:  tensor(-0.8009, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8055, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7999, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7986, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8000, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 13\n",
      "Step:  13 Dimension:  8\n",
      "Loss is:  tensor(-0.8007, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8044, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7995, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7979, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7996, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 13\n",
      "Step:  13 Dimension:  9\n",
      "Loss is:  tensor(-0.7996, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8022, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7967, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7947, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7964, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 13\n",
      "Step:  13 Dimension:  10\n",
      "Loss is:  tensor(-0.7988, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8029, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7978, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7962, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8012, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 13\n",
      "Step:  13 Dimension:  11\n",
      "Loss is:  tensor(-0.7972, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.8027, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-0.7968, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7956, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7967, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 13\n",
      "Step:  13 Dimension:  12\n",
      "Loss is:  tensor(-0.7928, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7981, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7923, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7917, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7932, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 13\n",
      "Step:  13 Dimension:  13\n",
      "Loss is:  tensor(-0.7911, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7975, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7905, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7887, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7902, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 13\n",
      "Step:  13 Dimension:  14\n",
      "Loss is:  tensor(-0.7909, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7964, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7913, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7914, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7928, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 13\n",
      "Step:  13 Dimension:  15\n",
      "Loss is:  tensor(-0.7892, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7951, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7900, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7879, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7901, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 13\n",
      "Step:  13 Dimension:  16\n",
      "Loss is:  tensor(-0.7907, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7936, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7884, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7868, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7898, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 13\n",
      "Step:  13 Dimension:  17\n",
      "Loss is:  tensor(-0.7927, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7968, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7878, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7854, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7910, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 13\n",
      "Step:  13 Dimension:  18\n",
      "Loss is:  tensor(-0.7922, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7960, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7888, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7859, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7920, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 13\n",
      "Seconds for Iteration 13 : 754.0842015743256\n",
      "Step:  14 Dimension:  0\n",
      "Loss is:  tensor(-0.7885, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7935, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7877, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7886, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7901, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 14\n",
      "Step:  14 Dimension:  1\n",
      "Loss is:  tensor(-0.7892, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7959, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7886, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7875, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7912, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 14\n",
      "Step:  14 Dimension:  2\n",
      "Loss is:  tensor(-0.7898, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7926, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7892, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7856, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7894, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 14\n",
      "Step:  14 Dimension:  3\n",
      "Loss is:  tensor(-0.7897, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7929, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7849, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7875, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7897, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 14\n",
      "Step:  14 Dimension:  4\n",
      "Loss is:  tensor(-0.7854, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7883, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7834, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7831, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7867, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 14\n",
      "Step:  14 Dimension:  5\n",
      "Loss is:  tensor(-0.7838, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7884, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7840, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7824, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7847, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 14\n",
      "Step:  14 Dimension:  6\n",
      "Loss is:  tensor(-0.7834, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7893, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7834, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7805, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7849, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 14\n",
      "Step:  14 Dimension:  7\n",
      "Loss is:  tensor(-0.7835, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7872, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7822, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7804, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7838, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 14\n",
      "Step:  14 Dimension:  8\n",
      "Loss is:  tensor(-0.7799, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7842, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7792, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7783, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7798, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 14\n",
      "Step:  14 Dimension:  9\n",
      "Loss is:  tensor(-0.7815, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7835, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7807, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7786, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7824, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 14\n",
      "Step:  14 Dimension:  10\n",
      "Loss is:  tensor(-0.7816, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7862, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7815, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7793, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7827, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 14\n",
      "Step:  14 Dimension:  11\n",
      "Loss is:  tensor(-0.7829, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7869, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7819, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7788, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7797, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 14\n",
      "Step:  14 Dimension:  12\n",
      "Loss is:  tensor(-0.7809, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7869, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7810, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7795, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7813, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 14\n",
      "Step:  14 Dimension:  13\n",
      "Loss is:  tensor(-0.7799, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7850, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7780, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7798, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7821, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 14\n",
      "Step:  14 Dimension:  14\n",
      "Loss is:  tensor(-0.7794, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7810, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7762, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7756, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7793, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 14\n",
      "Step:  14 Dimension:  15\n",
      "Loss is:  tensor(-0.7814, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7863, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7802, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7782, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7791, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 14\n",
      "Step:  14 Dimension:  16\n",
      "Loss is:  tensor(-0.7827, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7871, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7795, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7778, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7815, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 14\n",
      "Step:  14 Dimension:  17\n",
      "Loss is:  tensor(-0.7806, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7841, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7812, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7788, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7814, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 14\n",
      "Step:  14 Dimension:  18\n",
      "Loss is:  tensor(-0.7818, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7881, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7804, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7820, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7843, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 14\n",
      "Seconds for Iteration 14 : 790.471287727356\n",
      "Step:  15 Dimension:  0\n",
      "Loss is:  tensor(-0.7838, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7884, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7840, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7801, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-0.7860, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 15\n",
      "Step:  15 Dimension:  1\n",
      "Loss is:  tensor(-0.7873, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7929, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7850, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7834, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7866, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 15\n",
      "Step:  15 Dimension:  2\n",
      "Loss is:  tensor(-0.7883, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7915, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7874, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7840, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7874, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 15\n",
      "Step:  15 Dimension:  3\n",
      "Loss is:  tensor(-0.7860, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7922, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7862, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7836, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7866, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 15\n",
      "Step:  15 Dimension:  4\n",
      "Loss is:  tensor(-0.7873, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7894, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7858, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7837, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7874, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 15\n",
      "Step:  15 Dimension:  5\n",
      "Loss is:  tensor(-0.7828, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7883, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7836, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7830, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7859, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 15\n",
      "Step:  15 Dimension:  6\n",
      "Loss is:  tensor(-0.7848, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7883, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7865, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7813, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7857, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 15\n",
      "Step:  15 Dimension:  7\n",
      "Loss is:  tensor(-0.7824, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7886, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7842, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7820, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7834, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 15\n",
      "Step:  15 Dimension:  8\n",
      "Loss is:  tensor(-0.7823, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7875, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7818, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7803, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7847, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 15\n",
      "Step:  15 Dimension:  9\n",
      "Loss is:  tensor(-0.7826, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7883, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7831, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7811, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7834, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 15\n",
      "Step:  15 Dimension:  10\n",
      "Loss is:  tensor(-0.7833, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7889, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7834, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7807, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7844, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 15\n",
      "Step:  15 Dimension:  11\n",
      "Loss is:  tensor(-0.7849, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7920, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7865, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7808, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7880, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 15\n",
      "Step:  15 Dimension:  12\n",
      "Loss is:  tensor(-0.7871, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7900, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7857, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7834, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7880, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 15\n",
      "Step:  15 Dimension:  13\n",
      "Loss is:  tensor(-0.7838, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7888, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7836, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7820, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7861, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 15\n",
      "Step:  15 Dimension:  14\n",
      "Loss is:  tensor(-0.7774, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7855, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7772, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7759, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7791, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 15\n",
      "Step:  15 Dimension:  15\n",
      "Loss is:  tensor(-0.7773, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7832, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7769, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7753, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7760, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 15\n",
      "Step:  15 Dimension:  16\n",
      "Loss is:  tensor(-0.7749, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7782, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7741, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7706, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7762, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 15\n",
      "Step:  15 Dimension:  17\n",
      "Loss is:  tensor(-0.7740, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7778, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7711, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7703, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7729, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 15\n",
      "Step:  15 Dimension:  18\n",
      "Loss is:  tensor(-0.7719, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7759, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7701, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7686, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7735, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 15\n",
      "Seconds for Iteration 15 : 829.3937802314758\n",
      "Step:  16 Dimension:  0\n",
      "Loss is:  tensor(-0.7743, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7794, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7749, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7730, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7760, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 16\n",
      "Step:  16 Dimension:  1\n",
      "Loss is:  tensor(-0.7751, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7800, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7732, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7706, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7767, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 16\n",
      "Step:  16 Dimension:  2\n",
      "Loss is:  tensor(-0.7789, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7828, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7760, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7748, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7765, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 16\n",
      "Step:  16 Dimension:  3\n",
      "Loss is:  tensor(-0.7778, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7814, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7748, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7760, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7773, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 16\n",
      "Step:  16 Dimension:  4\n",
      "Loss is:  tensor(-0.7761, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7811, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7727, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7704, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7742, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 16\n",
      "Step:  16 Dimension:  5\n",
      "Loss is:  tensor(-0.7746, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7791, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7753, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7716, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7743, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 16\n",
      "Step:  16 Dimension:  6\n",
      "Loss is:  tensor(-0.7746, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7795, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7744, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7726, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7750, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 16\n",
      "Step:  16 Dimension:  7\n",
      "Loss is:  tensor(-0.7752, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7797, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7757, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7733, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7752, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 16\n",
      "Step:  16 Dimension:  8\n",
      "Loss is:  tensor(-0.7765, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7820, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7740, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7723, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7769, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 16\n",
      "Step:  16 Dimension:  9\n",
      "Loss is:  tensor(-0.7752, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-0.7813, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7753, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7739, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7765, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 16\n",
      "Step:  16 Dimension:  10\n",
      "Loss is:  tensor(-0.7764, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7807, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7736, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7727, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7767, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 16\n",
      "Step:  16 Dimension:  11\n",
      "Loss is:  tensor(-0.7747, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7784, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7729, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7729, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7747, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 16\n",
      "Step:  16 Dimension:  12\n",
      "Loss is:  tensor(-0.7744, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7785, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7712, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7708, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7732, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 16\n",
      "Step:  16 Dimension:  13\n",
      "Loss is:  tensor(-0.7707, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7787, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7730, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7715, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7737, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 16\n",
      "Step:  16 Dimension:  14\n",
      "Loss is:  tensor(-0.7689, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7753, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7677, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7650, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7684, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 16\n",
      "Step:  16 Dimension:  15\n",
      "Loss is:  tensor(-0.7664, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7692, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7661, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7623, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7661, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 16\n",
      "Step:  16 Dimension:  16\n",
      "Loss is:  tensor(-0.7691, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7755, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7694, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7686, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7692, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 16\n",
      "Step:  16 Dimension:  17\n",
      "Loss is:  tensor(-0.7670, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7720, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7663, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7653, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7663, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 16\n",
      "Step:  16 Dimension:  18\n",
      "Loss is:  tensor(-0.7663, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7702, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7656, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7638, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7659, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 16\n",
      "Seconds for Iteration 16 : 806.4015181064606\n",
      "Step:  17 Dimension:  0\n",
      "Loss is:  tensor(-0.7678, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7732, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7669, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7654, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7680, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 17\n",
      "Step:  17 Dimension:  1\n",
      "Loss is:  tensor(-0.7741, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7790, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7728, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7699, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7739, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 17\n",
      "Step:  17 Dimension:  2\n",
      "Loss is:  tensor(-0.7756, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7799, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7747, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7739, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7753, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 17\n",
      "Step:  17 Dimension:  3\n",
      "Loss is:  tensor(-0.7709, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7805, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7689, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7701, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7745, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 17\n",
      "Step:  17 Dimension:  4\n",
      "Loss is:  tensor(-0.7728, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7750, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7720, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7673, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7717, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 17\n",
      "Step:  17 Dimension:  5\n",
      "Loss is:  tensor(-0.7687, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7757, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7689, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7672, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7711, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 17\n",
      "Step:  17 Dimension:  6\n",
      "Loss is:  tensor(-0.7703, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7758, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7691, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7658, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7715, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 17\n",
      "Step:  17 Dimension:  7\n",
      "Loss is:  tensor(-0.7687, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7759, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7693, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7653, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7694, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 17\n",
      "Step:  17 Dimension:  8\n",
      "Loss is:  tensor(-0.7717, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7759, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7716, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7691, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7725, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 17\n",
      "Step:  17 Dimension:  9\n",
      "Loss is:  tensor(-0.7693, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7754, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7707, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7665, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7708, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 17\n",
      "Step:  17 Dimension:  10\n",
      "Loss is:  tensor(-0.7705, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7755, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7707, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7681, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7722, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 17\n",
      "Step:  17 Dimension:  11\n",
      "Loss is:  tensor(-0.7714, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7765, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7697, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7674, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7726, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 17\n",
      "Step:  17 Dimension:  12\n",
      "Loss is:  tensor(-0.7732, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7765, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7713, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7693, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7730, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 17\n",
      "Step:  17 Dimension:  13\n",
      "Loss is:  tensor(-0.7695, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7772, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7711, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7677, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7721, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 17\n",
      "Step:  17 Dimension:  14\n",
      "Loss is:  tensor(-0.7723, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7759, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7704, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7702, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7731, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 17\n",
      "Step:  17 Dimension:  15\n",
      "Loss is:  tensor(-0.7708, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7769, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7708, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7689, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7719, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 17\n",
      "Step:  17 Dimension:  16\n",
      "Loss is:  tensor(-0.7721, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7768, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7716, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7688, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7695, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 17\n",
      "Step:  17 Dimension:  17\n",
      "Loss is:  tensor(-0.7722, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7783, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7713, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7705, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-0.7719, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 17\n",
      "Step:  17 Dimension:  18\n",
      "Loss is:  tensor(-0.7722, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7767, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7690, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7684, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7715, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 17\n",
      "Seconds for Iteration 17 : 774.7650010585785\n",
      "Step:  18 Dimension:  0\n",
      "Loss is:  tensor(-0.7723, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7754, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7700, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7692, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7721, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 18\n",
      "Step:  18 Dimension:  1\n",
      "Loss is:  tensor(-0.7705, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7768, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7680, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7694, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7705, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 18\n",
      "Step:  18 Dimension:  2\n",
      "Loss is:  tensor(-0.7674, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7729, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7674, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7656, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7700, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 18\n",
      "Step:  18 Dimension:  3\n",
      "Loss is:  tensor(-0.7656, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7738, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7632, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7620, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7656, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 18\n",
      "Step:  18 Dimension:  4\n",
      "Loss is:  tensor(-0.7657, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7683, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7636, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7618, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7634, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 18\n",
      "Step:  18 Dimension:  5\n",
      "Loss is:  tensor(-0.7627, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7693, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7595, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7588, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7623, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 18\n",
      "Step:  18 Dimension:  6\n",
      "Loss is:  tensor(-0.7633, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7645, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7615, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7582, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7605, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 18\n",
      "Step:  18 Dimension:  7\n",
      "Loss is:  tensor(-0.7559, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7609, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7545, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7536, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7559, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 18\n",
      "Step:  18 Dimension:  8\n",
      "Loss is:  tensor(-0.7632, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7672, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7602, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7591, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7617, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 18\n",
      "Step:  18 Dimension:  9\n",
      "Loss is:  tensor(-0.7620, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7676, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7608, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7585, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7619, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 18\n",
      "Step:  18 Dimension:  10\n",
      "Loss is:  tensor(-0.7612, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7652, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7602, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7574, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7605, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 18\n",
      "Step:  18 Dimension:  11\n",
      "Loss is:  tensor(-0.7604, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7670, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7597, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7575, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7616, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 18\n",
      "Step:  18 Dimension:  12\n",
      "Loss is:  tensor(-0.7616, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7653, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7597, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7567, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7607, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 18\n",
      "Step:  18 Dimension:  13\n",
      "Loss is:  tensor(-0.7593, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7684, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7614, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7587, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7616, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 18\n",
      "Step:  18 Dimension:  14\n",
      "Loss is:  tensor(-0.7640, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7680, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7607, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7595, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7617, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 18\n",
      "Step:  18 Dimension:  15\n",
      "Loss is:  tensor(-0.7647, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7684, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7636, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7625, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7646, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 18\n",
      "Step:  18 Dimension:  16\n",
      "Loss is:  tensor(-0.7628, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7677, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7627, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7601, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7626, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 18\n",
      "Step:  18 Dimension:  17\n",
      "Loss is:  tensor(-0.7659, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7721, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7639, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7637, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7652, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 18\n",
      "Step:  18 Dimension:  18\n",
      "Loss is:  tensor(-0.7689, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7728, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7677, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7643, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7701, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 18\n",
      "Seconds for Iteration 18 : 741.7034237384796\n",
      "Step:  19 Dimension:  0\n",
      "Loss is:  tensor(-0.7644, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7694, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7623, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7613, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7634, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 19\n",
      "Step:  19 Dimension:  1\n",
      "Loss is:  tensor(-0.7616, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7676, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7616, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7593, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7632, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 19\n",
      "Step:  19 Dimension:  2\n",
      "Loss is:  tensor(-0.7589, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7676, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7582, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7592, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7606, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 19\n",
      "Step:  19 Dimension:  3\n",
      "Loss is:  tensor(-0.7606, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7684, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7602, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7583, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7618, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 19\n",
      "Step:  19 Dimension:  4\n",
      "Loss is:  tensor(-0.7619, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7676, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7593, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7593, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7623, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 19\n",
      "Step:  19 Dimension:  5\n",
      "Loss is:  tensor(-0.7619, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7679, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7614, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7597, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7627, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 19\n",
      "Step:  19 Dimension:  6\n",
      "Loss is:  tensor(-0.7615, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7643, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7586, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7575, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7608, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 19\n",
      "Step:  19 Dimension:  7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  tensor(-0.7589, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7638, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7570, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7539, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7599, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 19\n",
      "Step:  19 Dimension:  8\n",
      "Loss is:  tensor(-0.7570, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7623, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7538, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7518, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7574, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 19\n",
      "Step:  19 Dimension:  9\n",
      "Loss is:  tensor(-0.7547, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7622, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7539, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7520, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7563, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 19\n",
      "Step:  19 Dimension:  10\n",
      "Loss is:  tensor(-0.7555, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7630, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7526, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7549, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7586, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 19\n",
      "Step:  19 Dimension:  11\n",
      "Loss is:  tensor(-0.7614, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7642, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7582, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7557, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7585, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 19\n",
      "Step:  19 Dimension:  12\n",
      "Loss is:  tensor(-0.7608, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7645, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7587, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7556, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7593, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 19\n",
      "Step:  19 Dimension:  13\n",
      "Loss is:  tensor(-0.7553, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7646, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7562, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7568, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7584, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 19\n",
      "Step:  19 Dimension:  14\n",
      "Loss is:  tensor(-0.7568, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7663, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7577, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7547, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7592, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 19\n",
      "Step:  19 Dimension:  15\n",
      "Loss is:  tensor(-0.7653, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7708, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7630, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7593, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7652, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 19\n",
      "Step:  19 Dimension:  16\n",
      "Loss is:  tensor(-0.7645, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7708, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7628, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7595, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7620, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 19\n",
      "Step:  19 Dimension:  17\n",
      "Loss is:  tensor(-0.7641, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7708, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7630, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7613, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7654, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 19\n",
      "Step:  19 Dimension:  18\n",
      "Loss is:  tensor(-0.7648, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7696, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7624, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7597, grad_fn=<DivBackward0>)\n",
      "Loss is:  tensor(-0.7640, grad_fn=<DivBackward0>)\n",
      "Task: 0 ; Iteration 19\n",
      "Seconds for Iteration 19 : 748.3363764286041\n"
     ]
    }
   ],
   "source": [
    "alt_sampler.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.4061741665684985\n",
      "NLL: 173.87383310060534\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "d = model(test_x).mean - test_y\n",
    "\n",
    "test_rmse = torch.sqrt(torch.mean(torch.pow(d, 2)))\n",
    "print(\"Test RMSE: {}\".format(test_rmse))\n",
    "\n",
    "y_preds = likelihood(model(test_x))\n",
    "y_var = y_preds.variance\n",
    "\n",
    "nll = 0.5 * torch.log(2. * math.pi * y_var) +  torch.pow((model(test_x).mean - test_y),2)/(2. * y_var)\n",
    "nll_sum = nll.sum()\n",
    "print(\"NLL: {}\".format(nll_sum))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 100])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_sampler.gsampled[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Spectral Densities by Dimension, Skillcraft')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = ['navy', 'orange', 'teal', 'red', 'royalblue', 'goldenrod', 'firebrick', 'chartreuse',\n",
    "         'dodgerblue', 'green', 'orchid', 'peru', 'dimgray', 'salmon', 'indigo', 'thistle',\n",
    "         'skyblue', 'olive', 'moccasin']\n",
    "\n",
    "for dim in range(in_dims):\n",
    "    # get last 10 samples for current dimension\n",
    "    dim_samples = alt_sampler.gsampled[dim][0, :,-10:]\n",
    "    \n",
    "    omega = model.covar_module.get_omega(dim).cpu().numpy()\n",
    "    \n",
    "    plt.plot(omega, dim_samples.exp().cpu().numpy(), color=colors[dim], alpha = 0.15)\n",
    "plt.grid()\n",
    "\n",
    "plt.xlabel('Omega', fontsize=16)\n",
    "plt.ylabel('S(Omega)', fontsize = 16)\n",
    "plt.xlim((0, 4))\n",
    "plt.ylim((0, 4))\n",
    "plt.title('Spectral Densities by Dimension, Skillcraft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
